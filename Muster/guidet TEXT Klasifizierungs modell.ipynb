{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee62fcb0-fbfd-4bd5-af37-8547f56f95c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee62fcb0-fbfd-4bd5-af37-8547f56f95c3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808767032,
     "user_tz": -60,
     "elapsed": 6254,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    },
    "outputId": "6766e27b-2ff3-4dae-f42f-b723f472a0f3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np  # for numerical operations\n",
    "import pandas as pd  # for data manipulation\n",
    "import random  # for shuffling the data\n",
    "import nltk\n",
    "import re  # for handling regular expressions\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer  # for lemmatizing words\n",
    "from nltk.corpus import stopwords  # for stop word removal\n",
    "from nltk.tokenize import word_tokenize  # for tokenizing sentences into words\n",
    "\n",
    "# Downloading necessary NLTK resources\n",
    "nltk.download('stopwords')  # List of common stop words in English\n",
    "nltk.download('punkt')  # Pre-trained tokenizer models\n",
    "nltk.download('wordnet')  # WordNet lemmatizer dataset\n",
    "nltk.download('punkt_tab')  # Downloads the 'punkt' tokenizer table used for tokenization of text into sentences or words\n",
    "\n",
    "# Libraries for text feature extraction and model training\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Convert text into numerical features (TF-IDF)\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression for classification\n",
    "from sklearn.svm import LinearSVC  # Support Vector Machines for classification\n",
    "\n",
    "# Libraries for model evaluation\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix  # For model evaluation metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score  # For cross-validation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5637c05d-10a4-4551-88d8-8de53d7db8a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5637c05d-10a4-4551-88d8-8de53d7db8a1",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808767799,
     "user_tz": -60,
     "elapsed": 763,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    },
    "outputId": "a9706718-18a8-4c7a-c5fb-c7fbbe591df9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                   0\n",
      "0                  simplistic , silly and tedious . \n",
      "1  it's so laddish and juvenile , only teenage bo...\n",
      "2  exploitative and largely devoid of the depth o...\n",
      "3  [garbus] discards the potential for pathologic...\n",
      "4  a visually flashy but narratively opaque and e...\n"
     ]
    }
   ],
   "source": [
    "# Read the positive and negative sentiment files\n",
    "df_sent_pos = pd.read_csv('https://raw.githubusercontent.com/chrisvdweth/nus-cs4248x/refs/heads/master/1-foundations/data/corpora/sentence-polarity-dataset/sentence-polarity.neg', sep='\\t', header=None)  # Positive sentiment sentences\n",
    "df_sent_neg = pd.read_csv('https://raw.githubusercontent.com/chrisvdweth/nus-cs4248x/refs/heads/master/1-foundations/data/corpora/sentence-polarity-dataset/sentence-polarity.pos', sep='\\t', header=None)  # Negative sentiment sentences\n",
    "\n",
    "# Display the first few rows of the positive dataset to understand its structure\n",
    "print(df_sent_pos.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e929ce68-fe35-48ce-80c9-43e764ec4ae7",
   "metadata": {
    "id": "e929ce68-fe35-48ce-80c9-43e764ec4ae7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808767847,
     "user_tz": -60,
     "elapsed": 41,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Rename the column to 'sentence'\n",
    "df_sent_pos.rename(columns={0: \"sentence\"}, inplace=True)\n",
    "df_sent_neg.rename(columns={0: \"sentence\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3080d66d-2e4e-4a85-a093-911e832dee1c",
   "metadata": {
    "id": "3080d66d-2e4e-4a85-a093-911e832dee1c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808767897,
     "user_tz": -60,
     "elapsed": 20,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_text(sentences):\n",
    "    # Convert all tokens to lowercase\n",
    "    sentences = [sentence.lower() for sentence in sentences]\n",
    "\n",
    "    # Remove punctuation using regex\n",
    "    sentences = [re.sub(r\"[^\\w\\s]\", \"\", sentence) for sentence in sentences]\n",
    "\n",
    "    # Remove extra whitespaces between words\n",
    "    sentences = [\" \".join(sentence.split()) for sentence in sentences]\n",
    "\n",
    "    # Tokenize sentences into words\n",
    "    sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))  # Load English stop words\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        filtered_sentence = [word for word in sentence if word not in stop_words]\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentences = []\n",
    "    for sentence in filtered_sentences:\n",
    "        lemmatized_sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "        lemmatized_sentences.append(lemmatized_sentence)\n",
    "\n",
    "    return [' '.join(sentence) for sentence in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86ae052d-b9cd-490b-a05b-d8e22bec6326",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86ae052d-b9cd-490b-a05b-d8e22bec6326",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808785586,
     "user_tz": -60,
     "elapsed": 17674,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    },
    "outputId": "a9d98947-dc67-4062-ec57-40e070074906"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "rock destined 21st century new conan he going make splash even greater arnold schwarzenegger jeanclaud van damme steven segal\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the sentences\n",
    "pos_preprocessed_sentences = preprocess_text(df_sent_pos['sentence'])\n",
    "neg_preprocessed_sentences = preprocess_text(df_sent_neg['sentence'])\n",
    "\n",
    "# Print the first preprocessed negative sentence\n",
    "print(neg_preprocessed_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aba38a9b-eef8-41a0-ad06-e15961bcbb28",
   "metadata": {
    "id": "aba38a9b-eef8-41a0-ad06-e15961bcbb28",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808785590,
     "user_tz": -60,
     "elapsed": 3,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Combine preprocessed positive and negative sentences\n",
    "sentences = pos_preprocessed_sentences + neg_preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2c32ba9-a00b-4021-95ad-565f00bd685f",
   "metadata": {
    "id": "e2c32ba9-a00b-4021-95ad-565f00bd685f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808785604,
     "user_tz": -60,
     "elapsed": 12,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create a list for all labels\n",
    "polarities = []\n",
    "polarities.extend([0] * len(df_sent_neg))  # Label negative sentences as 0\n",
    "polarities.extend([1] * len(df_sent_pos))  # Label positive sentences as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f7c9d3-5084-4c58-b1fd-b3efb212313f",
   "metadata": {
    "id": "24f7c9d3-5084-4c58-b1fd-b3efb212313f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808785619,
     "user_tz": -60,
     "elapsed": 13,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Combine sentences and labels into a single list\n",
    "combined = list(zip(sentences, polarities))\n",
    "\n",
    "# Shuffle the combined list\n",
    "random.shuffle(combined)\n",
    "\n",
    "# Split the shuffled list back into sentences and labels\n",
    "sentences[:], polarities[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff035d59-dfb8-4e64-a94c-fbad9d63eafa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff035d59-dfb8-4e64-a94c-fbad9d63eafa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808785631,
     "user_tz": -60,
     "elapsed": 5,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    },
    "outputId": "3292b72e-f94b-4f3b-e916-13a927f36d6d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of training set: 8529\n",
      "Size of test set: 2133\n"
     ]
    }
   ],
   "source": [
    "# Define train-test split ratio\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "# Calculate the size of the training set\n",
    "train_set_size = int(train_test_ratio * len(sentences))\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test = sentences[:train_set_size], sentences[train_set_size:]\n",
    "y_train, y_test = polarities[:train_set_size], polarities[train_set_size:]\n",
    "\n",
    "# Print sizes of training and test sets\n",
    "print(\"Size of training set:\", len(X_train))\n",
    "print(\"Size of test set:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83d1e90b-ae28-44cd-9c54-8d6e7b91e97c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83d1e90b-ae28-44cd-9c54-8d6e7b91e97c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808785934,
     "user_tz": -60,
     "elapsed": 301,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    },
    "outputId": "bcfcd58f-1f5b-49a0-824d-e23837d157f0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#Samples: 8529, #Features: 8529\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "num_samples, num_features = X_train_tfidf.shape\n",
    "print(\"#Samples: {}, #Features: {}\".format(num_samples, num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d52db49e-3327-41fb-9c6c-177e5a853562",
   "metadata": {
    "id": "d52db49e-3327-41fb-9c6c-177e5a853562",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808786084,
     "user_tz": -60,
     "elapsed": 147,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import the Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the Logistic Regression classifier\n",
    "logistic_regression_classifier = LogisticRegression(solver=\"sag\").fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1c9a2c9-aa92-42cd-9da1-6bac6e68ece7",
   "metadata": {
    "id": "b1c9a2c9-aa92-42cd-9da1-6bac6e68ece7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808786173,
     "user_tz": -60,
     "elapsed": 80,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Transform the test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1dab2d3-c623-434d-bca3-497bda721f69",
   "metadata": {
    "id": "f1dab2d3-c623-434d-bca3-497bda721f69",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808786177,
     "user_tz": -60,
     "elapsed": 2,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Predict polarities for the test data\n",
    "y_pred = logistic_regression_classifier.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "affe1fad-3adb-4aa8-bdfb-bd343ca45cc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "affe1fad-3adb-4aa8-bdfb-bd343ca45cc3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808786215,
     "user_tz": -60,
     "elapsed": 36,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    },
    "outputId": "03951ae3-d375-4383-ee1c-8ef1ad213529"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77      1073\n",
      "           1       0.76      0.77      0.77      1060\n",
      "\n",
      "    accuracy                           0.77      2133\n",
      "   macro avg       0.77      0.77      0.77      2133\n",
      "weighted avg       0.77      0.77      0.77      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import evaluation metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Generate the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de0e212b-9311-49d7-adee-d90fe690b747",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de0e212b-9311-49d7-adee-d90fe690b747",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808792290,
     "user_tz": -60,
     "elapsed": 6074,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    },
    "outputId": "21f02200-8597-4d16-842c-866e710847e6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 Scores for each fold: [0.76363636 0.73659118 0.76540284 0.74545455 0.74450867 0.75177305\n",
      " 0.75458716 0.74495848 0.73388042 0.7587822 ]\n",
      "F1 Score (Mean/Average): 0.750\n",
      "F1 Score (Standard Deviation): 0.010\n"
     ]
    }
   ],
   "source": [
    "# Import necessary library\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Perform 10-fold cross-validation on the training data\n",
    "f1_scores_list = cross_val_score(\n",
    "    LogisticRegression(),            # Model: Logistic Regression\n",
    "    X_train_tfidf,                   # Features: TF-IDF transformed training data\n",
    "    y_train,                         # Labels: Training labels\n",
    "    cv=10,                           # Number of folds\n",
    "    scoring=\"f1\"                     # Evaluation metric: F1 score\n",
    ")\n",
    "\n",
    "# Display the F1 scores for each fold\n",
    "print(f\"F1 Scores for each fold: {f1_scores_list}\")\n",
    "\n",
    "# Calculate and display the mean and standard deviation of the F1 scores\n",
    "print(\"F1 Score (Mean/Average): {:.3f}\".format(f1_scores_list.mean()))\n",
    "print(\"F1 Score (Standard Deviation): {:.3f}\".format(f1_scores_list.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76c183ba-0f53-46ab-b856-18aa45319861",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76c183ba-0f53-46ab-b856-18aa45319861",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808805317,
     "user_tz": -60,
     "elapsed": 13025,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    },
    "outputId": "c4778742-c921-48eb-f733-d86ee69c2779"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Classifier: LinearSVC, n-gram size: 1 => F1-score: 0.743\n",
      "Classifier: LinearSVC, n-gram size: 2 => F1-score: 0.757\n",
      "Classifier: LinearSVC, n-gram size: 3 => F1-score: 0.752\n",
      "Classifier: LinearSVC, n-gram size: 4 => F1-score: 0.752\n",
      "Classifier: LogisticRegression, n-gram size: 1 => F1-score: 0.751\n",
      "Classifier: LogisticRegression, n-gram size: 2 => F1-score: 0.745\n",
      "Classifier: LogisticRegression, n-gram size: 3 => F1-score: 0.739\n",
      "Classifier: LogisticRegression, n-gram size: 4 => F1-score: 0.737\n",
      "\n",
      "Best Configuration:\n",
      "Classifier: LinearSVC, Max n-gram size: 2, F1-score: 0.757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize placeholders to store the best configuration\n",
    "best_score = -1.0\n",
    "best_classifier = None\n",
    "best_ngram_size = -1\n",
    "\n",
    "# Define the hyperparameters to test\n",
    "classifiers = [LinearSVC(), LogisticRegression(solver=\"sag\")]\n",
    "ngram_sizes = [1, 2, 3, 4]\n",
    "\n",
    "# Loop through all combinations of classifiers and n-gram sizes\n",
    "for classifier in classifiers:\n",
    "    for n in ngram_sizes:\n",
    "        # Define the vectorizer with the current n-gram size\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, n))\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)  # Transform training data\n",
    "\n",
    "        # Perform 10-fold cross-validation\n",
    "        f1_scores = cross_val_score(classifier, X_train_tfidf, y_train, cv=10, scoring='f1')\n",
    "        avg_f1_score = f1_scores.mean()  # Calculate average F1-score\n",
    "\n",
    "        # Print the result for this combination\n",
    "        print(f\"Classifier: {type(classifier).__name__}, n-gram size: {n} => F1-score: {avg_f1_score:.3f}\")\n",
    "\n",
    "        # Save the best configuration\n",
    "        if avg_f1_score > best_score:\n",
    "            best_score = avg_f1_score\n",
    "            best_classifier = classifier\n",
    "            best_ngram_size = n\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"\\nBest Configuration:\")\n",
    "print(f\"Classifier: {type(best_classifier).__name__}, Max n-gram size: {best_ngram_size}, F1-score: {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03370708-7017-41de-9b10-53868357cee6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03370708-7017-41de-9b10-53868357cee6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765808805852,
     "user_tz": -60,
     "elapsed": 489,
     "user": {
      "displayName": "Agus Luigi",
      "userId": "04718890403591418896"
     }
    },
    "outputId": "2cf77d37-0713-492c-ffce-cbd3f897e311"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Final Model Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.77      1073\n",
      "           1       0.77      0.79      0.78      1060\n",
      "\n",
      "    accuracy                           0.78      2133\n",
      "   macro avg       0.78      0.78      0.78      2133\n",
      "weighted avg       0.78      0.78      0.78      2133\n",
      "\n",
      "Accuracy: 0.776\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Use the best configuration to train the final model\n",
    "final_vectorizer = TfidfVectorizer(ngram_range=(1, best_ngram_size))\n",
    "X_train_tfidf = final_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = final_vectorizer.transform(X_test)\n",
    "\n",
    "best_classifier.fit(X_train_tfidf, y_train)\n",
    "y_pred = best_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate and display results\n",
    "print(\"\\nFinal Model Results:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "colab": {
   "provenance": [
    {
     "file_id": "1QUNvH5YVEVdVXvl2BRo0OaeKckNkoOa6",
     "timestamp": 1765808562001
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
